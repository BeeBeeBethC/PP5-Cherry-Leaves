{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "* Clean and remove non images.\n",
    "* Prepare the dataset for further processes.\n",
    "* Find average image shape for original images.\n",
    "* Resize the images from the datasets.\n",
    "* Split the datasets into Train, Validation and Test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Generate:\n",
    "'inputs/leaves_dataset/cherry-leaves/resized_images'\n",
    "'inputs/leaves_dataset/cherry-leaves/processed_images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No Additional Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/PP5-Cherry-Leaves/jupyter_notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You set a new current directory\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/PP5-Cherry-Leaves'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks for Corrupt Images and other image types that are not '.png', '.jpg' and '.jpeg' Image Types. This helps to prevent duplicates and conflicts with different image versions (i.e original images and resized images) ready for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: powdery_mildew - Valid Resized Images: 0, Removed: 2104\n",
      "Folder: healthy - Valid Resized Images: 0, Removed: 2104\n",
      "Folder: resized_images - Valid Resized Images: 0, Removed: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "def remove_invalid_and_original_images(my_data_dir, resized_suffix=\"resized\"):\n",
    "    image_extensions = ('.png', '.jpg', '.jpeg')\n",
    "    folders = os.listdir(my_data_dir)\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(my_data_dir, folder)\n",
    "        \n",
    "        if not os.path.isdir(folder_path):  \n",
    "            continue  # Skip if not a folder\n",
    "\n",
    "        i, j = 0, 0  # Counters\n",
    "\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            if os.path.isfile(file_path):\n",
    "                if not file_name.lower().endswith(image_extensions):  \n",
    "                    os.remove(file_path)  # Remove non-image files\n",
    "                    i += 1\n",
    "                elif resized_suffix not in file_name:  # Remove original-sized images\n",
    "                    os.remove(file_path)\n",
    "                    i += 1\n",
    "                else:\n",
    "                    try:\n",
    "                        # Try opening with TensorFlow to verify it's an actual image\n",
    "                        img = tf.io.read_file(file_path)\n",
    "                        img = tf.io.decode_image(img, channels=3)\n",
    "                        j += 1  # Valid resized image\n",
    "                    except Exception as e:\n",
    "                        print(f\"Removing corrupted image: {file_path} - Error: {str(e)}\")\n",
    "                        os.remove(file_path)\n",
    "                        i += 1\n",
    "\n",
    "        print(f\"Folder: {folder} - Valid Resized Images: {j}, Removed: {i}\")\n",
    "\n",
    "# Example usage:\n",
    "data_dir = '/workspaces/PP5-Cherry-Leaves/inputs/leaves_dataset/cherry-leaves'\n",
    "remove_invalid_and_original_images(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Resizing using ImageDataGenerator prints batch size, image shape and RGB presence for visual clarification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4208 images belonging to 2 classes.\n",
      "(10, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an ImageDataGenerator instance\n",
    "datagen = ImageDataGenerator(rescale=1./255)  # Normalization (optional)\n",
    "\n",
    "# Load images from a directory and resize them to 100x100\n",
    "data_generator = datagen.flow_from_directory(\n",
    "    '/workspaces/PP5-Cherry-Leaves/inputs/leaves_dataset/cherry-leaves/resized_images',  # Replace with your image directory\n",
    "    target_size=(100, 100),  # Resize to 100x100\n",
    "    batch_size=10,\n",
    "    class_mode='binary'  # Change based on your task (e.g., 'binary' for 2 classes)\n",
    ")\n",
    "\n",
    "# Fetch a batch of images and labels\n",
    "images, labels = next(data_generator)\n",
    "\n",
    "# Check the shape\n",
    "print(images.shape)  # Should be (batch_size, 100, 100, 3) for RGB images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving images to 'processed_images' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4208 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed images saved to /workspaces/PP5-Cherry-Leaves/inputs/leaves_dataset/cherry-leaves/processed_images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "import numpy as np\n",
    "\n",
    "# Define source and destination directories\n",
    "source_dir = '/workspaces/PP5-Cherry-Leaves/inputs/leaves_dataset/cherry-leaves/resized_images'  # Use resized images as source\n",
    "save_dir = '/workspaces/PP5-Cherry-Leaves/inputs/leaves_dataset/cherry-leaves/processed_images'  # Save final processed images\n",
    "\n",
    "# Create ImageDataGenerator for further processing (if needed)\n",
    "datagen = ImageDataGenerator(rescale=1./255)  # Normalize the images\n",
    "\n",
    "# Create the save directory if it doesn't exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Set up the generator with target size and batch size\n",
    "data_generator = datagen.flow_from_directory(\n",
    "    source_dir,\n",
    "    target_size=(100, 100),  # Resize to 100x100 (if necessary again)\n",
    "    batch_size=32,\n",
    "    class_mode='binary',  # Adjust if needed\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Ensure subdirectories exist for each class\n",
    "for class_name in data_generator.class_indices:\n",
    "    class_dir = os.path.join(save_dir, class_name)\n",
    "    if not os.path.exists(class_dir):\n",
    "        os.makedirs(class_dir)\n",
    "\n",
    "# Iterate over the batches of images and save them\n",
    "batch_index = 0\n",
    "for batch in data_generator:\n",
    "    images, labels = batch\n",
    "    for i, img in enumerate(images):\n",
    "        # Convert the image array to a PIL image\n",
    "        pil_img = array_to_img(img)\n",
    "        \n",
    "        # Determine the class name based on the label\n",
    "        label = labels[i] \n",
    "        class_name = list(data_generator.class_indices.keys())[int(label)]\n",
    "        \n",
    "        # Create a filename\n",
    "        img_filename = f\"processed_image_{batch_index * data_generator.batch_size + i}.png\"\n",
    "        \n",
    "        # Save the image in the appropriate class subdirectory\n",
    "        pil_img.save(os.path.join(save_dir, class_name, img_filename))\n",
    "    \n",
    "    batch_index += 1\n",
    "\n",
    "    # Stop after processing all images\n",
    "    if batch_index * data_generator.batch_size >= data_generator.samples:\n",
    "        break\n",
    "\n",
    "print(f\"Processed images saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into train, validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the 'processed_image' data into train, validation and test subsets to a ration of 0.7, 0.15, 0.15 summing up to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into train, validation, and test sets.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def split_train_validation_test_images(my_data_dir, train_set_ratio, validation_set_ratio, test_set_ratio):\n",
    "    if train_set_ratio + validation_set_ratio + test_set_ratio != 1.0:\n",
    "        print(\"train_set_ratio + validation_set_ratio + test_set_ratio should sum to 1.0\")\n",
    "        return\n",
    "\n",
    "    # Get class labels\n",
    "    labels = os.listdir(my_data_dir)\n",
    "    if 'test' in labels:\n",
    "        pass\n",
    "    else:\n",
    "        # Create train, validation, and test folders with class labels as subfolders\n",
    "        for folder in ['train', 'validation', 'test']:\n",
    "            for label in labels:\n",
    "                os.makedirs(name=os.path.join(my_data_dir, folder, label), exist_ok=True)\n",
    "\n",
    "        for label in labels:\n",
    "            files = os.listdir(os.path.join(my_data_dir, label))\n",
    "            random.shuffle(files)\n",
    "\n",
    "            train_set_files_qty = int(len(files) * train_set_ratio)\n",
    "            validation_set_files_qty = int(len(files) * validation_set_ratio)\n",
    "\n",
    "            count = 1\n",
    "            for file_name in files:\n",
    "                if count <= train_set_files_qty:\n",
    "                    # Move a file to the train set\n",
    "                    shutil.move(os.path.join(my_data_dir, label, file_name),\n",
    "                                os.path.join(my_data_dir, 'train', label, file_name))\n",
    "\n",
    "                elif count <= (train_set_files_qty + validation_set_files_qty):\n",
    "                    # Move a file to the validation set\n",
    "                    shutil.move(os.path.join(my_data_dir, label, file_name),\n",
    "                                os.path.join(my_data_dir, 'validation', label, file_name))\n",
    "\n",
    "                else:\n",
    "                    # Move a file to the test set\n",
    "                    shutil.move(os.path.join(my_data_dir, label, file_name),\n",
    "                                os.path.join(my_data_dir, 'test', label, file_name))\n",
    "\n",
    "                count += 1\n",
    "\n",
    "            # Remove empty class directories after splitting\n",
    "            os.rmdir(os.path.join(my_data_dir, label))\n",
    "\n",
    "# Split the resized images into train, validation, and test sets\n",
    "split_train_validation_test_images(save_dir, train_set_ratio=0.7, validation_set_ratio=0.15, test_set_ratio=0.15)\n",
    "\n",
    "print(f\"Data split into train, validation, and test sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comma separated values (CSV) files generated to store tabular data using plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from linx02 - genderpredictor project to create and save dataframes.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def create_dataframe(data_dir):\n",
    "    data = {'file': [], 'label': []}\n",
    "    for label in os.listdir(data_dir):\n",
    "        label_dir = os.path.join(data_dir, label)\n",
    "        for file in os.listdir(label_dir):\n",
    "            file_path = os.path.join(label_dir, file)\n",
    "            data['file'].append(file_path)\n",
    "            data['label'].append(label)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "data_dir = \"/workspaces/PP5-Cherry-Leaves/inputs/leaves_dataset/cherry-leaves/processed_images\"\n",
    "\n",
    "# Create DataFrames for train, validation, and test datasets\n",
    "train_df = create_dataframe(os.path.join(data_dir, 'train'))\n",
    "validation_df = create_dataframe(os.path.join(data_dir, 'validation'))\n",
    "test_df = create_dataframe(os.path.join(data_dir, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame saved as /workspaces/PP5-Cherry-Leaves/outputs/train_dataframe.csv\n",
      "Validation DataFrame saved as /workspaces/PP5-Cherry-Leaves/outputs/validation_dataframe.csv\n",
      "Test DataFrame saved as /workspaces/PP5-Cherry-Leaves/outputs/test_dataframe.csv\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/workspaces/PP5-Cherry-Leaves/outputs/\"\n",
    "\n",
    "dataframes = {'train': train_df, 'validation': validation_df, 'test': test_df}\n",
    "for dataset, df in dataframes.items():\n",
    "    csv_path = os.path.join(output_dir, f\"{dataset}_dataframe.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"{dataset.capitalize()} DataFrame saved as {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
